{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805fd240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, random, gc, sys, time\n",
    "import itertools\n",
    "from itertools import cycle\n",
    "from typing import Any, Sequence, Tuple, Optional, Iterable\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "import sentencepiece as spm\n",
    "import numpy as np\n",
    "from torch.amp import GradScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd0db80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Runtime / Performance Configurations ==========\n",
    "USE_CHECKPOINTING = False\n",
    "CHECKPOINT_ATTENTION_ONLY = False\n",
    "ACCUM_STEPS = 1\n",
    "BATCH_SIZE = 16\n",
    "SEQ_LEN = 512\n",
    "NUM_WORKERS = 6\n",
    "PIN_MEMORY = True\n",
    "PERSISTENT_WORKERS = True\n",
    "MOE_CHUNK_SIZE = 32768\n",
    "MOE_NUM_EXPERTS = 32\n",
    "MOE_TOP_K = 6\n",
    "MOE_CAPACITY_FACTOR = 2.5  \n",
    "MOE_AUX_LAMBDA = 0.0025  \n",
    "OOM_RETRY_LIMIT = 3\n",
    "EPOCHS = 5\n",
    "STEPS_PER_EPOCH = 50000\n",
    "LOG_INTERVAL = 50\n",
    "EVAL_INTERVAL = 200\n",
    "STORY_GEN_INTERVAL = 1000\n",
    "COMPARISON_GRAPH_INTERVAL = 5000\n",
    "VOCAB_SIZE = 8000\n",
    "n_embd = 768\n",
    "n_head = 8\n",
    "n_layer = 8\n",
    "dropout = 0.0\n",
    "learning_rate = 6e-4  \n",
    "weight_decay = 0.1\n",
    "warmup_ratio = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf6e961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Utility Functions ==========\n",
    "def cleanup_memory():\n",
    "    try:\n",
    "        torch.cuda.empty_cache()\n",
    "    except Exception:\n",
    "        pass\n",
    "    gc.collect()\n",
    "\n",
    "def set_seed(seed: int = 1337):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.set_device(0)\n",
    "        return torch.device(\"cuda:0\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "try:\n",
    "    torch.set_float32_matmul_precision('high')\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96a7ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Mixed Precision Manager ==========\n",
    "class MixedPrecisionManager:\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "        self.use_bf16 = (device.type == \"cuda\")\n",
    "        self.use_fp16 = False\n",
    "        self.use_fp32 = False\n",
    "        self.scaler = None\n",
    "        if self.use_bf16:\n",
    "            self._info = \"start: bf16 (no GradScaler)\"\n",
    "        else:\n",
    "            self.use_fp32 = True\n",
    "            self._info = \"start: fp32\"\n",
    "\n",
    "    def autocast_dtype(self):\n",
    "        if self.use_bf16 and self.device.type == \"cuda\":\n",
    "            return torch.bfloat16\n",
    "        elif self.use_fp16 and self.device.type == \"cuda\":\n",
    "            return torch.float16\n",
    "        else:\n",
    "            return torch.float32\n",
    "\n",
    "    def enable_fp16_with_scaler(self):\n",
    "        self.use_bf16 = False\n",
    "        self.use_fp16 = True\n",
    "        self.use_fp32 = False\n",
    "        self.scaler = GradScaler(enabled=(self.device.type == \"cuda\"))\n",
    "        self._info = \"switched to fp16 + GradScaler\"\n",
    "\n",
    "    def enable_fp32(self):\n",
    "        self.use_bf16 = False\n",
    "        self.use_fp16 = False\n",
    "        self.use_fp32 = True\n",
    "        self.scaler = None\n",
    "        self._info = \"switched to fp32\"\n",
    "\n",
    "    def info(self):\n",
    "        return self._info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76956052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== NanoKimiK2 Components ==========\n",
    "\n",
    "# SwiGLU Module\n",
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, in_features, out_features=None):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, out_features, bias=True)\n",
    "        self.fc2 = nn.Linear(in_features, out_features, bias=True)\n",
    "    def forward(self, x):\n",
    "        return F.silu(self.fc1(x)) * self.fc2(x)\n",
    "\n",
    "# Expert Module\n",
    "class Expert(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features, bias=True)\n",
    "        self.act = SwiGLU(hidden_features, hidden_features)\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features, bias=True)\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.act(self.fc1(x)))\n",
    "\n",
    "# Mixture of Experts (MoE) Layer\n",
    "class MoELayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features,\n",
    "                 num_experts=MOE_NUM_EXPERTS, top_k=MOE_TOP_K,\n",
    "                 capacity_factor=MOE_CAPACITY_FACTOR, chunk_size=MOE_CHUNK_SIZE):\n",
    "        super().__init__()\n",
    "        assert out_features == in_features, \"Residual-friendly MoE requires d_in == d_out.\"\n",
    "        self.d = in_features\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        self.capacity_factor = capacity_factor\n",
    "        self.chunk_size = chunk_size\n",
    "        self.entropy_lambda = 0.05\n",
    "\n",
    "        self.gate = nn.Linear(in_features, num_experts, bias=False)\n",
    "        self.experts = nn.ModuleList([Expert(in_features, in_features * 4, out_features) for _ in range(num_experts)])\n",
    "        self.shared_expert = Expert(in_features, in_features * 4, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        x_flat = x.reshape(B*T, C)\n",
    "        device = x.device\n",
    "        N = x_flat.size(0)\n",
    "        e = self.num_experts\n",
    "        d = self.d\n",
    "        k = self.top_k\n",
    "\n",
    "        logits = self.gate(x_flat)  # (N, e)\n",
    "        S = F.softmax(logits, dim=1)  # (N, e)\n",
    "\n",
    "        # Transpose to expert-centric view\n",
    "        S_T = S.transpose(0, 1)  # (e, N)\n",
    "\n",
    "        # Compute capacity per expert\n",
    "        capacity = int(self.capacity_factor * (N / e))\n",
    "\n",
    "        # Select top-capacity tokens per expert\n",
    "        G, I = torch.topk(S_T, capacity, dim=1)  # G (e, capacity), I (e, capacity)\n",
    "\n",
    "        # Create permutation matrix P (one-hot)\n",
    "        P = torch.zeros(e, capacity, N, dtype=x_flat.dtype, device=device)\n",
    "        P.scatter_(2, I.unsqueeze(2), 1)  # (e, capacity, N)\n",
    "\n",
    "        # Gather inputs for experts\n",
    "        X_in = torch.bmm(P, x_flat.unsqueeze(0).repeat(e, 1, 1))  # (e, capacity, d)\n",
    "\n",
    "        # Compute expert outputs\n",
    "        X_e = torch.zeros(e, capacity, d, dtype=x_flat.dtype, device=device)\n",
    "        for i in range(e):\n",
    "            X_e[i] = self.experts[i](X_in[i])\n",
    "\n",
    "        # Aggregate: Weight by G and permute back\n",
    "        weighted_X_e = G.unsqueeze(2) * X_e  # (e, capacity, d)\n",
    "        out_flat = torch.einsum('ecd,ecn->nd', weighted_X_e, P)  # (N, d)\n",
    "\n",
    "        # Add shared expert\n",
    "        out_flat += self.shared_expert(x_flat)\n",
    "\n",
    "        # Aux loss \n",
    "        importance = S.mean(dim=0)  # (e,)\n",
    "        load = (I >= 0).sum(dim=1).float() / N  # (e,)\n",
    "        imp_cv = importance.std() / (importance.mean() + 1e-12)\n",
    "        load_cv = load.std() / (load.mean() + 1e-12)\n",
    "        entropy = -(S * torch.log(S + 1e-12)).sum(dim=-1).mean()\n",
    "        aux_loss = e * (imp_cv + load_cv) + self.entropy_lambda * entropy\n",
    "\n",
    "        # Stats \n",
    "        stats = {\n",
    "            \"importance_cv\": float(imp_cv.item()),\n",
    "            \"load_cv\": float(load_cv.item()),\n",
    "            \"overflow_pct\": 0.0,\n",
    "            \"dropped_pct\": 0.0\n",
    "        }\n",
    "\n",
    "        return out_flat.view(B, T, C), aux_loss, stats\n",
    "\n",
    "# RoPE Cache Builder\n",
    "def build_rope_cache(max_seq_len: int, head_dim: int, device: torch.device):\n",
    "    inv_freq = 1.0 / (10000 ** (torch.arange(0, head_dim, 2, device=device).float() / head_dim))\n",
    "    positions = torch.arange(0, max_seq_len, device=device).float()\n",
    "    freqs = torch.einsum(\"i,j->ij\", positions, inv_freq)\n",
    "    emb = torch.cat((freqs, freqs), dim=-1)\n",
    "    cos = torch.cos(emb).to(device)\n",
    "    sin = torch.sin(emb).to(device)\n",
    "    return cos, sin\n",
    "\n",
    "# RoPE Application\n",
    "def apply_rope(x: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor):\n",
    "    B,H,T,D = x.shape\n",
    "    cos = cos[:T, :].unsqueeze(0).unsqueeze(0)\n",
    "    sin = sin[:T, :].unsqueeze(0).unsqueeze(0)\n",
    "    x1 = x[..., ::2]\n",
    "    x2 = x[..., 1::2]\n",
    "    x_rot_even = x1 * cos[..., ::2] - x2 * sin[..., ::2]\n",
    "    x_rot_odd  = x1 * sin[..., ::2] + x2 * cos[..., ::2]\n",
    "    x_out = torch.stack((x_rot_even, x_rot_odd), dim=-1).flatten(-2)\n",
    "    return x_out\n",
    "\n",
    "# MLAAttention\n",
    "class MLAAttention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_dim: int,\n",
    "                 num_heads: int,\n",
    "                 q_lora_rank: int = 192,\n",
    "                 kv_lora_rank: int = 32,\n",
    "                 rope_dim: int = 32,\n",
    "                 head_dim: int = 224,\n",
    "                 dropout: float = 0.0,\n",
    "                 max_seq_len: int = SEQ_LEN):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.q_lora_rank = q_lora_rank\n",
    "        self.kv_lora_rank = kv_lora_rank\n",
    "        self.rope_dim = rope_dim\n",
    "        assert head_dim == (q_lora_rank + rope_dim), \"head_dim must equal q_lora_rank + rope_dim.\"\n",
    "\n",
    "        self.q_a_proj = nn.Linear(embed_dim, q_lora_rank, bias=False)\n",
    "        self.q_b_proj = nn.Linear(q_lora_rank, num_heads * head_dim, bias=False)\n",
    "        self.kv_a_proj = nn.Linear(embed_dim, kv_lora_rank, bias=False)\n",
    "        self.kv_b_proj = nn.Linear(kv_lora_rank, num_heads * head_dim * 2, bias=False)\n",
    "        self.out_proj = nn.Linear(num_heads * head_dim, embed_dim, bias=False)\n",
    "        self.dropout_p = dropout\n",
    "\n",
    "        cos, sin = build_rope_cache(max_seq_len, rope_dim, device=torch.device(\"cpu\"))\n",
    "        self.register_buffer(\"rope_cos\", cos, persistent=False)\n",
    "        self.register_buffer(\"rope_sin\", sin, persistent=False)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def _shape_heads(self, x: torch.Tensor, B: int, T: int, D: int):\n",
    "        return x.view(B, T, self.num_heads, D).transpose(1, 2).contiguous()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = None):\n",
    "        B, T, C = x.shape\n",
    "        device = x.device\n",
    "        dtype = x.dtype\n",
    "\n",
    "        q = self.q_b_proj(F.silu(self.q_a_proj(x)))\n",
    "        q = q.view(B, T, self.num_heads, self.head_dim)\n",
    "        kv = self.kv_b_proj(F.silu(self.kv_a_proj(x)))\n",
    "        kv = kv.view(B, T, self.num_heads, self.head_dim * 2)\n",
    "        k, v = kv.chunk(2, dim=-1)\n",
    "\n",
    "        q_nope = q[..., :self.q_lora_rank]\n",
    "        q_pe = q[..., self.q_lora_rank:]\n",
    "        k_nope = k[..., :self.q_lora_rank]\n",
    "        k_pe = k[..., self.q_lora_rank:]\n",
    "        v = v\n",
    "\n",
    "        cos = self.rope_cos.to(device)\n",
    "        sin = self.rope_sin.to(device)\n",
    "        q_pe = apply_rope(q_pe, cos, sin)\n",
    "        k_pe = apply_rope(k_pe, cos, sin)\n",
    "\n",
    "        q = torch.cat([q_nope, q_pe], dim=-1)\n",
    "        k = torch.cat([k_nope, k_pe], dim=-1)\n",
    "\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "        scale = math.sqrt(self.head_dim)\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / scale\n",
    "        if attn_mask is not None:\n",
    "            scores = scores.masked_fill(attn_mask == 0, float('-inf'))\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.attn_dropout(attn)\n",
    "        y = torch.matmul(attn, v)\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, self.num_heads * self.head_dim)\n",
    "        y = self.out_proj(y)\n",
    "        return y\n",
    "\n",
    "# Transformer Block\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, max_seq_len,\n",
    "                 q_lora_rank=192, kv_lora_rank=32, rope_dim=32, head_dim=224, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.attn = MLAAttention(embed_dim, num_heads,\n",
    "                                 q_lora_rank=q_lora_rank, kv_lora_rank=kv_lora_rank,\n",
    "                                 rope_dim=rope_dim, head_dim=head_dim,\n",
    "                                 dropout=dropout, max_seq_len=max_seq_len)\n",
    "        self.moe = MoELayer(embed_dim, embed_dim,\n",
    "                            num_experts=MOE_NUM_EXPERTS,\n",
    "                            top_k=MOE_TOP_K,\n",
    "                            capacity_factor=MOE_CAPACITY_FACTOR)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        aux_loss = torch.tensor(0.0, device=x.device, dtype=x.dtype)\n",
    "\n",
    "        def attn_fn(h):\n",
    "            return self.attn(self.norm1(h))\n",
    "        if self.training and USE_CHECKPOINTING:\n",
    "            attn_out = torch.utils.checkpoint.checkpoint(attn_fn, x, use_reentrant=True, preserve_rng_state=False)\n",
    "        else:\n",
    "            attn_out = attn_fn(x)\n",
    "        x = x + attn_out\n",
    "\n",
    "        def moe_fn(h):\n",
    "            return self.moe(h)\n",
    "        if self.training and USE_CHECKPOINTING:\n",
    "            moe_out_tuple = torch.utils.checkpoint.checkpoint(lambda h: moe_fn(h), self.norm2(x), use_reentrant=True, preserve_rng_state=False)\n",
    "        else:\n",
    "            moe_out_tuple = moe_fn(self.norm2(x))\n",
    "\n",
    "        if isinstance(moe_out_tuple, tuple) and len(moe_out_tuple) == 3:\n",
    "            moe_out, moe_aux, moe_stats = moe_out_tuple\n",
    "        elif isinstance(moe_out_tuple, tuple) and len(moe_out_tuple) == 2:\n",
    "            moe_out, moe_aux = moe_out_tuple\n",
    "            moe_stats = {}\n",
    "        else:\n",
    "            moe_out = moe_out_tuple\n",
    "            moe_aux = torch.tensor(0.0, device=x.device, dtype=x.dtype)\n",
    "            moe_stats = {}\n",
    "\n",
    "        x = x + moe_out\n",
    "        aux_loss = aux_loss + moe_aux\n",
    "        return x, aux_loss, moe_stats\n",
    "\n",
    "# NanoKimi Transformer Model\n",
    "class NanoKimiTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=768, max_length=512, num_layers=8, num_heads=8,\n",
    "                 q_lora_rank=192, kv_lora_rank=32, rope_dim=32, head_dim=224, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.max_length = max_length\n",
    "        self.token_embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, max_seq_len=max_length,\n",
    "                             q_lora_rank=q_lora_rank, kv_lora_rank=kv_lora_rank,\n",
    "                             rope_dim=rope_dim, head_dim=head_dim, dropout=dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm_f = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        h = self.token_embed(x)\n",
    "        total_aux = torch.tensor(0.0, device=h.device, dtype=h.dtype)\n",
    "        total_importance_cv = 0.0\n",
    "        total_load_cv = 0.0\n",
    "        total_overflow = 0.0\n",
    "        stats_count = 0\n",
    "        for blk in self.blocks:\n",
    "            h, aux, stats = blk(h)\n",
    "            total_aux = total_aux + aux\n",
    "            if isinstance(stats, dict) and stats:\n",
    "                total_importance_cv += stats.get(\"importance_cv\", 0.0)\n",
    "                total_load_cv += stats.get(\"load_cv\", 0.0)\n",
    "                total_overflow += stats.get(\"overflow_pct\", 0.0)\n",
    "                stats_count += 1\n",
    "        h = self.norm_f(h)\n",
    "        logits = self.head(h)\n",
    "        agg_stats = {}\n",
    "        if stats_count > 0:\n",
    "            agg_stats = {\n",
    "                \"importance_cv\": total_importance_cv / stats_count,\n",
    "                \"load_cv\": total_load_cv / stats_count,\n",
    "                \"overflow_pct\": total_overflow / stats_count\n",
    "            }\n",
    "        else:\n",
    "            agg_stats = {\"importance_cv\": 0.0, \"load_cv\": 0.0, \"overflow_pct\": 0.0}\n",
    "        return logits, total_aux, agg_stats\n",
    "\n",
    "# Utility Functions\n",
    "def _add_identity(M, eps):\n",
    "    if M.ndim == 2:\n",
    "        return M + eps * torch.eye(M.size(0), device=M.device, dtype=M.dtype)\n",
    "    elif M.ndim == 3:\n",
    "        I = torch.eye(M.size(-1), device=M.device, dtype=M.dtype).expand(M.size(0), -1, -1)\n",
    "        return M + eps * I\n",
    "    return M\n",
    "\n",
    "@torch.no_grad()\n",
    "def _newton_schulz_inverse_pth_root(A, p=2, iters=5, eps=1e-6):\n",
    "    A = 0.5 * (A + A.transpose(-1, -2))\n",
    "    A = _add_identity(A, eps)\n",
    "    norm = A.norm(p='fro') + 1e-12\n",
    "    Y = A / norm\n",
    "    I = torch.eye(A.size(-1), device=A.device, dtype=A.dtype)\n",
    "    Z = torch.eye(A.size(-1), device=A.device, dtype=A.dtype)\n",
    "    for _ in range(iters):\n",
    "        T = (p + 1) * 0.5 * I - 0.5 * (Y @ Z + Z @ Y) / p\n",
    "        Y = Y @ T\n",
    "        Z = T @ Z\n",
    "    Z = Z * (norm ** (-1.0 / p))\n",
    "    return Z\n",
    "\n",
    "def _rms(x: torch.Tensor) -> torch.Tensor:\n",
    "    return (x.pow(2).mean()).sqrt()\n",
    "\n",
    "# Muon Optimizer\n",
    "class Muon(Optimizer):\n",
    "    def __init__(self,\n",
    "                 params,\n",
    "                 lr=6e-4,\n",
    "                 beta1=0.9, beta2=0.999,\n",
    "                 eps=1e-8,\n",
    "                 wd=0.1,\n",
    "                 ns_iters=5,\n",
    "                 precond_eps=1e-4,\n",
    "                 precond_update_freq=20,\n",
    "                 adam_bias_correction=True,\n",
    "                 qkclip_tau=20.0,\n",
    "                 qkclip_every=1,\n",
    "                 qkclip_calibrate_default=1.0,\n",
    "                 qkclip_decay=0.95):\n",
    "        defaults = dict(lr=lr, beta1=beta1, beta2=beta2, eps=eps, wd=wd,\n",
    "                        ns_iters=ns_iters, precond_eps=precond_eps,\n",
    "                        precond_update_freq=precond_update_freq,\n",
    "                        adam_bias_correction=adam_bias_correction,\n",
    "                        qkclip_tau=qkclip_tau, qkclip_every=qkclip_every,\n",
    "                        qkclip_calibrate_default=qkclip_calibrate_default,\n",
    "                        qkclip_decay=qkclip_decay)\n",
    "        super().__init__(params, defaults)\n",
    "        self._parent_modules = []\n",
    "        self._had_forward_once = False\n",
    "\n",
    "    def bind_modules(self, modules_list):\n",
    "        if not isinstance(modules_list, (list, tuple)):\n",
    "            raise ValueError(\"bind_modules expects list/tuple of modules\")\n",
    "        self._parent_modules = modules_list\n",
    "\n",
    "        for m in self._iter_attention_modules():\n",
    "            if not hasattr(m, \"_qkclip_rms\"):\n",
    "                m._qkclip_rms = None\n",
    "            if not hasattr(m, \"_qkclip_decay\"):\n",
    "                m._qkclip_decay = None\n",
    "\n",
    "            def _make_hook(mod):\n",
    "                def _hook(module, inputs):\n",
    "                    x = inputs[0]\n",
    "                    with torch.no_grad():\n",
    "                        val = _rms(x.detach()).to(x.dtype)\n",
    "                        if mod._qkclip_rms is None:\n",
    "                            mod._qkclip_rms = val.detach()\n",
    "                            mod._qkclip_decay = None\n",
    "                        else:\n",
    "                            decay = mod._qkclip_decay if mod._qkclip_decay is not None else 0.95\n",
    "                            mod._qkclip_rms.mul_(decay).add_(val, alpha=(1.0 - decay))\n",
    "                    return None\n",
    "                return _hook\n",
    "\n",
    "            if not hasattr(m, \"_qkclip_hook_handle\"):\n",
    "                m._qkclip_hook_handle = m.register_forward_pre_hook(_make_hook(m), with_kwargs=False)\n",
    "\n",
    "    def _iter_attention_modules(self):\n",
    "        for m in self._parent_modules:\n",
    "            if isinstance(m, MLAAttention):\n",
    "                yield m\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "            beta1, beta2 = group['beta1'], group['beta2']\n",
    "            eps = group['eps']\n",
    "            wd = group['wd']\n",
    "            ns_iters = group['ns_iters']\n",
    "            precond_eps = group['precond_eps']\n",
    "            precond_update_freq = group['precond_update_freq']\n",
    "            adam_bias_correction = group['adam_bias_correction']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                st = self.state[p]\n",
    "                if len(st) == 0:\n",
    "                    st['step'] = 0\n",
    "                    st['m'] = torch.zeros_like(p)\n",
    "                    st['v'] = torch.zeros_like(p)\n",
    "                    if p.ndim >= 2:\n",
    "                        st['G'] = torch.zeros((p.shape[-2], p.shape[-2]), device=p.device, dtype=p.dtype)\n",
    "                        st['H'] = torch.zeros((p.shape[-1], p.shape[-1]), device=p.device, dtype=p.dtype)\n",
    "\n",
    "                st['step'] += 1\n",
    "\n",
    "                if wd != 0:\n",
    "                    p.add_(p, alpha=-lr * wd)\n",
    "\n",
    "                g = p.grad\n",
    "                v = st['v']\n",
    "                v.mul_(beta2).addcmul_(g, g, value=1 - beta2)\n",
    "\n",
    "                if p.ndim >= 2:\n",
    "                    G = st['G']; H = st['H']\n",
    "                    G.mul_(beta2).addmm_(g, g.t(), beta=1 - beta2)\n",
    "                    H.mul_(beta2).addmm_(g.t(), g, beta=1 - beta2)\n",
    "\n",
    "                    if (st['step'] % precond_update_freq == 0) or ('G_inv_root' not in st):\n",
    "                        st['G_inv_root'] = _newton_schulz_inverse_pth_root(G, p=2, iters=ns_iters, eps=precond_eps)\n",
    "                        st['H_inv_root'] = _newton_schulz_inverse_pth_root(H, p=2, iters=ns_iters, eps=precond_eps)\n",
    "\n",
    "                    Ginv = st['G_inv_root']; Hinv = st['H_inv_root']\n",
    "                    g_pre = Ginv @ g @ Hinv\n",
    "                else:\n",
    "                    g_pre = g\n",
    "\n",
    "                if adam_bias_correction:\n",
    "                    t = st['step']\n",
    "                    v_hat = v / (1.0 - (beta2 ** t))\n",
    "                else:\n",
    "                    v_hat = v\n",
    "                denom = v_hat.sqrt().add_(eps)\n",
    "                g_adam_ref = g / denom\n",
    "\n",
    "                rms_pre = _rms(g_pre)\n",
    "                rms_ref = _rms(g_adam_ref)\n",
    "                scale = (rms_ref / (rms_pre + 1e-16)).clamp_(min=0.0, max=1e6)\n",
    "                g_muon = g_pre * scale\n",
    "\n",
    "                p.add_(g_muon, alpha=-lr)\n",
    "\n",
    "        self._had_forward_once = True\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            tau = group['qkclip_tau']\n",
    "            every = group['qkclip_every']\n",
    "            decay = group['qkclip_decay']\n",
    "            default_rms = group['qkclip_calibrate_default']\n",
    "\n",
    "            any_param = None\n",
    "            for p in group['params']:\n",
    "                if p in self.state:\n",
    "                    any_param = p\n",
    "                    break\n",
    "            if any_param is None:\n",
    "                continue\n",
    "            t = self.state[any_param].get('step', 1)\n",
    "            if (t % max(1, every)) != 0:\n",
    "                continue\n",
    "\n",
    "            for attn in self._iter_attention_modules():\n",
    "                if hasattr(attn, \"_qkclip_rms\") and attn._qkclip_rms is not None:\n",
    "                    attn._qkclip_decay = decay\n",
    "                    a_rms = float(attn._qkclip_rms.detach().item())\n",
    "                else:\n",
    "                    a_rms = float(default_rms)\n",
    "\n",
    "                H = attn.num_heads\n",
    "                dh = attn.head_dim\n",
    "\n",
    "                if hasattr(attn, \"q_b_proj\") and hasattr(attn, \"q_a_proj\") and hasattr(attn, \"kv_b_proj\") and hasattr(attn, \"kv_a_proj\"):\n",
    "                    W_qb = attn.q_b_proj.weight\n",
    "                    W_qa = attn.q_a_proj.weight\n",
    "                    W_kvb = attn.kv_b_proj.weight\n",
    "                    W_kva = attn.kv_a_proj.weight\n",
    "                    try:\n",
    "                        Mq = W_qb @ W_qa\n",
    "                        Mk = W_kvb @ W_kva\n",
    "                        used_composite = True\n",
    "                    except Exception:\n",
    "                        used_composite = False\n",
    "\n",
    "                if not used_composite:\n",
    "                    continue\n",
    "\n",
    "                for h in range(H):\n",
    "                    rs = h * dh\n",
    "                    re = rs + dh\n",
    "                    if rs >= Mq.size(0) or re > Mq.size(0):\n",
    "                        continue\n",
    "                    Mq_h = Mq[rs:re, :]\n",
    "                    Mk_h = Mk[rs:re, :]\n",
    "\n",
    "                    nq = torch.linalg.norm(Mq_h, ord='fro')\n",
    "                    nk = torch.linalg.norm(Mk_h, ord='fro')\n",
    "                    s_h = (float(nq.item()) * a_rms) * (float(nk.item()) * a_rms) / math.sqrt(max(1, dh))\n",
    "\n",
    "                    if s_h > tau and math.isfinite(s_h):\n",
    "                        alpha = math.sqrt(tau / s_h)\n",
    "                        attn.q_b_proj.weight[rs:re, :].mul_(alpha)\n",
    "                        attn.kv_b_proj.weight[rs:re, :].mul_(alpha)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01a258f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Data ==========\n",
    "\n",
    "# Story Dataset\n",
    "class StoryDataset(Dataset):\n",
    "    def __init__(self, tokens, max_length=512, stride=512):\n",
    "        self.max_length = max_length\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        if len(tokens) < max_length:\n",
    "            return\n",
    "        for i in range(0, max(1, len(tokens) - max_length) + 1, stride):\n",
    "            inp = tokens[i:i+max_length]\n",
    "            if len(inp) < max_length:\n",
    "                continue\n",
    "            tgt = tokens[i+1:i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(inp, dtype=torch.long))\n",
    "            self.target_ids.append(torch.tensor(tgt, dtype=torch.long))\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "# Collate Function\n",
    "def collate(batch: Sequence):\n",
    "    xs, ys = zip(*batch)\n",
    "    x = torch.stack([torch.as_tensor(xx, dtype=torch.long) for xx in xs])\n",
    "    y = torch.stack([torch.as_tensor(yy, dtype=torch.long) for yy in ys])\n",
    "    return x, y\n",
    "\n",
    "# DataLoader Creation\n",
    "def make_dataloaders(train_ds, val_ds, batch_size):\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=min(NUM_WORKERS, 8),\n",
    "                              pin_memory=PIN_MEMORY,\n",
    "                              persistent_workers=PERSISTENT_WORKERS,\n",
    "                              collate_fn=collate, drop_last=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size,\n",
    "                            shuffle=False,\n",
    "                            num_workers=min(NUM_WORKERS, 8),\n",
    "                            pin_memory=PIN_MEMORY,\n",
    "                            persistent_workers=PERSISTENT_WORKERS,\n",
    "                            collate_fn=collate, drop_last=False)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "# Bits Per Character (BPC) Computation\n",
    "def compute_bpc(nll, tokens, encoding, is_gpt=False):\n",
    "    if is_gpt:\n",
    "        text = ''.join([encoding[t.item()] for t in tokens.flatten()])\n",
    "    else:\n",
    "        text = encoding.decode(tokens.flatten().tolist())\n",
    "    num_chars = len(text)\n",
    "    num_tokens = tokens.numel()\n",
    "    return (nll * num_tokens) / max(1, num_chars) / math.log(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eacb31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Eval & Generation for NanoKimiK2 ==========\n",
    "\n",
    "# Evaluation Function\n",
    "@torch.no_grad()\n",
    "def evaluate_kimi(model, loader, device, mp_mgr, encoding):\n",
    "    model.eval()\n",
    "    total_nll_acc = 0.0\n",
    "    total_aux = 0.0\n",
    "    total_targets = 0\n",
    "    total_importance_cv = 0.0\n",
    "    total_load_cv = 0.0\n",
    "    total_overflow = 0.0\n",
    "    total_bpc = 0.0\n",
    "    count = 0\n",
    "    dtype = mp_mgr.autocast_dtype()\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        yb = yb.to(device, non_blocking=True)\n",
    "        with torch.amp.autocast(device_type=\"cuda\" if device.type==\"cuda\" else \"cpu\", dtype=dtype):\n",
    "            logits, aux, stats = model(xb)\n",
    "            nll = F.cross_entropy(logits.reshape(-1, logits.size(-1)), yb.reshape(-1))\n",
    "            _ = nll + MOE_AUX_LAMBDA * aux\n",
    "        num_targets = yb.numel()\n",
    "        total_nll_acc += float(nll.detach().cpu().item()) * num_targets\n",
    "        total_aux += float(aux.detach().cpu().item()) if torch.is_tensor(aux) else float(aux)\n",
    "        total_importance_cv += stats.get(\"importance_cv\", 0.0)\n",
    "        total_load_cv += stats.get(\"load_cv\", 0.0)\n",
    "        total_overflow += stats.get(\"overflow_pct\", 0.0)\n",
    "        bpc_batch = compute_bpc(float(nll.detach().cpu().item()), yb.cpu(), encoding, is_gpt=False)\n",
    "        total_bpc += bpc_batch\n",
    "        total_targets += num_targets\n",
    "        count += 1\n",
    "\n",
    "    if count == 0:\n",
    "        return float(\"inf\"), float(\"inf\"), float(\"inf\"), float(\"inf\"), {}\n",
    "\n",
    "    mean_nll = total_nll_acc / max(1, total_targets)\n",
    "    mean_aux = total_aux / count\n",
    "    perplexity = math.exp(mean_nll)\n",
    "    mean_bpc = total_bpc / count\n",
    "    avg_stats = {\n",
    "        \"importance_cv\": total_importance_cv / max(1, count),\n",
    "        \"load_cv\": total_load_cv / max(1, count),\n",
    "        \"overflow_pct\": total_overflow / max(1, count),\n",
    "        \"avg_aux\": mean_aux\n",
    "    }\n",
    "    torch.cuda.empty_cache()\n",
    "    return perplexity, mean_nll, mean_aux, mean_bpc, avg_stats\n",
    "\n",
    "# Generation Function\n",
    "@torch.no_grad()\n",
    "def generate_kimi(model, encoding, prompt, device, mp_mgr, max_tokens=200, temperature=1.2, top_k=100):\n",
    "    model.eval()\n",
    "    ids = encoding.encode(prompt, out_type=int)\n",
    "    x = torch.tensor(ids, dtype=torch.long, device=device).unsqueeze(0)\n",
    "    for _ in range(max_tokens):\n",
    "        dtype = mp_mgr.autocast_dtype()\n",
    "        with torch.amp.autocast(device_type=\"cuda\" if device.type==\"cuda\" else \"cpu\", dtype=dtype):\n",
    "            logits, _, _ = model(x)\n",
    "            logits = logits[:, -1, :] / max(1e-6, temperature)\n",
    "            if top_k is not None and top_k > 0:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                thresh = v[:, -1].unsqueeze(-1)\n",
    "                logits = torch.where(logits < thresh, torch.full_like(logits, -float('inf')), logits)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_id = torch.multinomial(probs, num_samples=1)\n",
    "        x = torch.cat([x, next_id], dim=1)\n",
    "        if x.size(1) > SEQ_LEN:\n",
    "            x = x[:, -SEQ_LEN:]\n",
    "    return encoding.decode(x[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60709394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== NanoGPT Components ==========\n",
    "\n",
    "# Attention Head\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, n_embd: int, head_size: int, block_size: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = head_size ** -0.5\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2, -1) * self.scale\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "# Multi-Head Attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_embd: int, num_heads: int, block_size: int, dropout: float):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // num_heads\n",
    "        self.heads = nn.ModuleList([Head(n_embd, head_size, block_size, dropout) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "# Feed-Forward Network\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Transformer Block\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd: int, n_head: int, block_size: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.sa = MultiHeadAttention(n_embd, n_head, block_size, dropout)\n",
    "        self.ffwd = FeedForward(n_embd, dropout)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# GPT Language Model\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int, block_size: int, n_embd: int, n_head: int, n_layer: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head, block_size, dropout) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "        self.block_size = block_size\n",
    "        self.apply(self._init_weights)\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    def forward(self, idx, targets: Optional[torch.Tensor] = None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=tok_emb.device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens: int, temperature: float = 1.2, top_k: Optional[int] = 100):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / max(1e-6, temperature)\n",
    "            if top_k is not None and top_k > 0:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                thresh = v[:, -1].unsqueeze(-1)\n",
    "                logits = torch.where(logits < thresh, torch.full_like(logits, -float('inf')), logits)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_id = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, next_id), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a4e4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Eval & Generation for NanoGPT ==========\n",
    "\n",
    "# Evaluation Function for NanoGPT\n",
    "@torch.no_grad()\n",
    "def evaluate_gpt(model, loader, device, mp_mgr, encoding):\n",
    "    model.eval()\n",
    "    total_nll_acc = 0.0\n",
    "    total_targets = 0\n",
    "    total_bpc = 0.0\n",
    "    count = 0\n",
    "    dtype = mp_mgr.autocast_dtype()\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        yb = yb.to(device, non_blocking=True)\n",
    "        with torch.amp.autocast(device_type=\"cuda\" if device.type==\"cuda\" else \"cpu\", dtype=dtype):\n",
    "            _, loss = model(xb, yb)\n",
    "            if loss is None:\n",
    "                continue\n",
    "            nll = loss\n",
    "        num_targets = yb.numel()\n",
    "        total_nll_acc += float(nll.detach().cpu().item()) * num_targets\n",
    "        bpc_batch = compute_bpc(float(nll.detach().cpu().item()), yb.cpu(), encoding, is_gpt=False)\n",
    "        total_bpc += bpc_batch\n",
    "        total_targets += num_targets\n",
    "        count += 1\n",
    "    if count == 0:\n",
    "        return float(\"inf\"), float(\"inf\"), float(\"inf\"), {}\n",
    "    mean_nll = total_nll_acc / max(1, total_targets)\n",
    "    perplexity = math.exp(mean_nll)\n",
    "    mean_bpc = total_bpc / count\n",
    "    stats = {\"mean_nll\": mean_nll}\n",
    "    return perplexity, mean_nll, mean_bpc, stats\n",
    "\n",
    "# Generation Function for NanoGPT\n",
    "@torch.no_grad()\n",
    "def generate_gpt(model, encoding, prompt, device, mp_mgr, max_tokens=200, temperature=1.2, top_k=100):\n",
    "    model.eval()\n",
    "    ids = encoding.encode(prompt, out_type=int)\n",
    "    x = torch.tensor(ids, dtype=torch.long, device=device).unsqueeze(0)\n",
    "    dtype = mp_mgr.autocast_dtype()\n",
    "    with torch.amp.autocast(device_type=\"cuda\" if device.type==\"cuda\" else \"cpu\", dtype=dtype):\n",
    "        sample_ids = model.generate(x, max_tokens, temperature, top_k)\n",
    "    return encoding.decode(sample_ids[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b518d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Scheduler ==========\n",
    "def get_cosine_with_warmup_scheduler(optimizer, warmup_steps, total_steps, min_multiplier=0.0):\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < warmup_steps and warmup_steps > 0:\n",
    "            return float(current_step) / float(max(1, warmup_steps))\n",
    "        progress = float(current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "        return max(min_multiplier, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621dfa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Aux multiplier for NanoKimiK2 ==========\n",
    "def compute_aux_multiplier(step: int, batch_size: int):\n",
    "    step = max(1, int(step))\n",
    "    bs_scale = math.log1p(max(1, batch_size))\n",
    "    step_scale = math.sqrt(math.log1p(step))\n",
    "    multiplier = min(1.0, max(0.5, (bs_scale * step_scale) / 10.0))\n",
    "    return multiplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6d4aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Main ==========\n",
    "\n",
    "# Main Training Function\n",
    "def main():\n",
    "    cleanup_memory()\n",
    "    set_seed(1337)\n",
    "\n",
    "    # Device Setup\n",
    "    device = get_device()\n",
    "    print(\"Device:\", device, \"| cuda available:\", torch.cuda.is_available())\n",
    "    if device.type == \"cuda\":\n",
    "        try:\n",
    "            print(torch.cuda.get_device_name(0))\n",
    "        except Exception:\n",
    "            pass\n",
    "        print(\"VRAM reserved (MB):\", torch.cuda.memory_reserved()/1e6)\n",
    "\n",
    "    # Load TinyStories Dataset\n",
    "    dataset = load_dataset(\"roneneldan/TinyStories\")\n",
    "    texts = [ex[\"text\"] for ex in dataset[\"train\"]]\n",
    "    with open(\"tinystories.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for line in texts:\n",
    "            f.write(line.strip() + \"\\n\")\n",
    "\n",
    "    # Tokenizer Training\n",
    "    def train_tokenizer(vocab_size=8000, model_type=\"unigram\"):\n",
    "        model_prefix = f\"spm_tinystories_{vocab_size}\"\n",
    "        spm.SentencePieceTrainer.Train(\n",
    "            input=\"tinystories.txt\",\n",
    "            model_prefix=model_prefix,\n",
    "            vocab_size=vocab_size,\n",
    "            model_type=model_type,\n",
    "            character_coverage=1.0,\n",
    "            byte_fallback=True,\n",
    "            unk_id=0, bos_id=1, eos_id=2, pad_id=3\n",
    "        )\n",
    "        print(f\"Tokenizer trained: {model_prefix}.model\")\n",
    "        return model_prefix + \".model\"\n",
    "\n",
    "    model_file = train_tokenizer(8000)\n",
    "    encoding = spm.SentencePieceProcessor(model_file=model_file)\n",
    "    train_tokens = [tok for ex in dataset[\"train\"] for tok in encoding.encode(ex[\"text\"], out_type=int)]\n",
    "    val_tokens = [tok for ex in dataset[\"validation\"] for tok in encoding.encode(ex[\"text\"], out_type=int)]\n",
    "    vocab_size = encoding.get_piece_size()\n",
    "\n",
    "    # Dataset Preparation\n",
    "    train_ds = StoryDataset(train_tokens, max_length=SEQ_LEN, stride=512)\n",
    "    val_ds = StoryDataset(val_tokens, max_length=SEQ_LEN, stride=512)\n",
    "    if len(train_ds) == 0:\n",
    "        raise RuntimeError(\"Train dataset empty â€” lower SEQ_LEN or check tokenization.\")\n",
    "\n",
    "    # Print General Information\n",
    "    print(\"General Information:\")\n",
    "    print(f\"  Context Length: {SEQ_LEN} tokens\")\n",
    "    print(f\"  Training Dataset Size: {len(train_ds)} samples\")\n",
    "    print(f\"  Validation Dataset Size: {len(val_ds)} samples\")\n",
    "    print(f\"  Vocabulary Size: {VOCAB_SIZE} tokens\")\n",
    "    print(f\"  Embedding dimension: {n_embd}\")\n",
    "    print(f\"  Dimension of input & Hidden layer: {n_embd}\")\n",
    "    print(f\"  Number of Epochs: {EPOCHS}\")\n",
    "    print(f\"  Batch Size: {BATCH_SIZE}\")\n",
    "    print(f\"  MoE Experts: {MOE_NUM_EXPERTS}\")\n",
    "    print(f\"  MoE Top-K: {MOE_TOP_K}\")\n",
    "    print(f\"  Train dataset size: {len(train_ds)} samples\")\n",
    "    print(f\"  Validation dataset size: {len(val_ds)} samples\")\n",
    "    print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "    print(f\"  Steps per epoch: {STEPS_PER_EPOCH}\")\n",
    "    print(f\"  Epochs: {EPOCHS}\")\n",
    "\n",
    "    # Mixed Precision Manager\n",
    "    mp_mgr = MixedPrecisionManager(device)\n",
    "\n",
    "    # Initialize NanoKimiK2 Model\n",
    "    model_kimi = NanoKimiTransformer(\n",
    "        vocab_size=vocab_size,\n",
    "        embed_dim=n_embd,\n",
    "        num_layers=n_layer,\n",
    "        num_heads=n_head,\n",
    "        q_lora_rank=192,\n",
    "        kv_lora_rank=32,\n",
    "        rope_dim=32,\n",
    "        head_dim=224,\n",
    "        dropout=dropout,\n",
    "        max_length=SEQ_LEN\n",
    "    ).to(device, dtype=(torch.bfloat16 if mp_mgr.use_bf16 and device.type==\"cuda\" else torch.float32))\n",
    "\n",
    "    # Initialize Muon Optimizer for NanoKimiK2\n",
    "    optimizer_kimi = Muon(\n",
    "        model_kimi.parameters(),\n",
    "        lr=learning_rate,\n",
    "        beta1=0.9,\n",
    "        beta2=0.999,\n",
    "        eps=1e-8,\n",
    "        wd=weight_decay,\n",
    "        ns_iters=5,\n",
    "        precond_eps=1e-4,\n",
    "        precond_update_freq=20,\n",
    "        adam_bias_correction=True,\n",
    "        qkclip_tau=20.0,\n",
    "        qkclip_every=1,\n",
    "        qkclip_calibrate_default=1.0,\n",
    "        qkclip_decay=0.95\n",
    "    )\n",
    "    optimizer_kimi.bind_modules([model_kimi])\n",
    "\n",
    "    # Initialize NanoGPT Model\n",
    "    model_gpt = GPTLanguageModel(\n",
    "        vocab_size=vocab_size,\n",
    "        block_size=SEQ_LEN,\n",
    "        n_embd=n_embd,\n",
    "        n_head=n_head,\n",
    "        n_layer=n_layer,\n",
    "        dropout=dropout\n",
    "    ).to(device, dtype=(torch.bfloat16 if mp_mgr.use_bf16 and device.type==\"cuda\" else torch.float32))\n",
    "\n",
    "    # Initialize AdamW Optimizer for NanoGPT\n",
    "    optimizer_gpt = torch.optim.AdamW(model_gpt.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # DataLoader Setup\n",
    "    current_batch = BATCH_SIZE\n",
    "    train_loader, val_loader = make_dataloaders(train_ds, val_ds, current_batch)\n",
    "\n",
    "    # Learning Rate Schedulers\n",
    "    total_steps = EPOCHS * STEPS_PER_EPOCH\n",
    "    warmup_steps = min(1000, total_steps//10)\n",
    "    scheduler_kimi = get_cosine_with_warmup_scheduler(optimizer_kimi, warmup_steps, total_steps)\n",
    "    scheduler_gpt = get_cosine_with_warmup_scheduler(optimizer_gpt, warmup_steps, total_steps)\n",
    "\n",
    "    # Print Training Configuration\n",
    "    print(f\"Train dataset size: {len(train_ds)} samples\")\n",
    "    print(f\"Validation dataset size: {len(val_ds)} samples\")\n",
    "    print(f\"Batch size: {current_batch}\")\n",
    "    print(f\"Steps per epoch: {STEPS_PER_EPOCH}\")\n",
    "    print(f\"Epochs: {EPOCHS}\")\n",
    "    print(f\"With eval every {EVAL_INTERVAL} steps.\\n\")\n",
    "\n",
    "    # Mixed Precision Setup\n",
    "    if mp_mgr.use_bf16:\n",
    "        scaler = None\n",
    "        print(\"Starting in bfloat16 (no GradScaler).\")\n",
    "    else:\n",
    "        scaler = GradScaler(enabled=(device.type==\"cuda\"))\n",
    "        print(\"Starting in fp32 with GradScaler as applicable.\")\n",
    "\n",
    "    print(\"Starting training loop. MoE experts:\", MOE_NUM_EXPERTS, \"top_k:\", MOE_TOP_K, \"chunk_size:\", MOE_CHUNK_SIZE)\n",
    "\n",
    "    # Training Loop\n",
    "    global_step = 0\n",
    "    first_run_done = False\n",
    "\n",
    "    eval_prompts = [\n",
    "        \"Once, there was a small boy named Charlie. Charlie loved stories more than anything else.\"\n",
    "    ]\n",
    "\n",
    "    # Lists for Tracking Losses\n",
    "    train_losses_kimi = []\n",
    "    train_losses_gpt = []\n",
    "    val_losses_kimi = []\n",
    "    val_losses_gpt = []\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model_kimi.train()\n",
    "        model_gpt.train()\n",
    "        running_loss_kimi = running_nll_kimi = running_aux_kimi = 0.0\n",
    "        running_aux_multiplier = 0.0\n",
    "        running_scaled_aux_kimi = 0.0\n",
    "        running_importance_cv_kimi = running_load_cv_kimi = running_overflow_kimi = 0.0\n",
    "        running_loss_gpt = 0.0\n",
    "        running_count = 0\n",
    "\n",
    "        train_iter = cycle(iter(train_loader))\n",
    "        optimizer_kimi.zero_grad(set_to_none=True)\n",
    "        optimizer_gpt.zero_grad(set_to_none=True)\n",
    "\n",
    "        for step_in_epoch in range(1, STEPS_PER_EPOCH + 1):\n",
    "            global_step += 1\n",
    "            attempt = 0\n",
    "            x, y = next(train_iter)\n",
    "\n",
    "            # Training Step with OOM Handling\n",
    "            while attempt <= OOM_RETRY_LIMIT:\n",
    "                try:\n",
    "                    x = x.to(device, non_blocking=True)\n",
    "                    y = y.to(device, non_blocking=True)\n",
    "\n",
    "                    dtype = mp_mgr.autocast_dtype()\n",
    "                    with torch.amp.autocast(device_type=\"cuda\" if device.type==\"cuda\" else \"cpu\", dtype=dtype):\n",
    "                        logits_kimi, aux_kimi, stats_kimi = model_kimi(x)\n",
    "                        nll_kimi = F.cross_entropy(logits_kimi.view(-1, logits_kimi.size(-1)), y.view(-1))\n",
    "                        eff_batch = x.size(0) * max(1, ACCUM_STEPS)\n",
    "                        aux_multiplier = compute_aux_multiplier(global_step, eff_batch)\n",
    "                        scaled_aux_kimi = aux_kimi * aux_multiplier\n",
    "                        loss_kimi = nll_kimi + MOE_AUX_LAMBDA * scaled_aux_kimi\n",
    "                        loss_kimi = loss_kimi / max(1, ACCUM_STEPS)\n",
    "\n",
    "                        _, loss_gpt = model_gpt(x, y)\n",
    "                        loss_gpt = loss_gpt / max(1, ACCUM_STEPS)\n",
    "\n",
    "                    if mp_mgr.use_fp16:\n",
    "                        if mp_mgr.scaler is None:\n",
    "                            mp_mgr.scaler = GradScaler(enabled=True)\n",
    "                        mp_mgr.scaler.scale(loss_kimi).backward()\n",
    "                        mp_mgr.scaler.scale(loss_gpt).backward()\n",
    "                    else:\n",
    "                        loss_kimi.backward()\n",
    "                        loss_gpt.backward()\n",
    "\n",
    "                    if (global_step % max(1, ACCUM_STEPS)) == 0:\n",
    "                        if mp_mgr.use_fp16:\n",
    "                            mp_mgr.scaler.unscale_(optimizer_kimi)\n",
    "                            mp_mgr.scaler.unscale_(optimizer_gpt)\n",
    "                        torch.nn.utils.clip_grad_norm_(model_kimi.parameters(), 1.0)\n",
    "                        torch.nn.utils.clip_grad_norm_(model_gpt.parameters(), 1.0)\n",
    "                        if mp_mgr.use_fp16:\n",
    "                            mp_mgr.scaler.step(optimizer_kimi)\n",
    "                            mp_mgr.scaler.step(optimizer_gpt)\n",
    "                            mp_mgr.scaler.update()\n",
    "                        else:\n",
    "                            optimizer_kimi.step()\n",
    "                            optimizer_gpt.step()\n",
    "                        scheduler_kimi.step()\n",
    "                        scheduler_gpt.step()\n",
    "                        optimizer_kimi.zero_grad(set_to_none=True)\n",
    "                        optimizer_gpt.zero_grad(set_to_none=True)\n",
    "\n",
    "                    reported_loss_kimi = float((loss_kimi * max(1, ACCUM_STEPS)).detach().cpu().item())\n",
    "                    train_losses_kimi.append(reported_loss_kimi)\n",
    "                    running_loss_kimi += reported_loss_kimi\n",
    "                    running_nll_kimi += float(nll_kimi.detach().cpu().item())\n",
    "                    running_aux_kimi += float(aux_kimi.detach().cpu().item()) if torch.is_tensor(aux_kimi) else float(aux_kimi)\n",
    "                    running_aux_multiplier += aux_multiplier\n",
    "                    running_scaled_aux_kimi += float(scaled_aux_kimi.detach().cpu().item())\n",
    "                    running_importance_cv_kimi += stats_kimi.get(\"importance_cv\", 0.0)\n",
    "                    running_load_cv_kimi += stats_kimi.get(\"load_cv\", 0.0)\n",
    "                    running_overflow_kimi += stats_kimi.get(\"overflow_pct\", 0.0)\n",
    "\n",
    "                    reported_loss_gpt = float((loss_gpt * max(1, ACCUM_STEPS)).detach().cpu().item())\n",
    "                    train_losses_gpt.append(reported_loss_gpt)\n",
    "                    running_loss_gpt += reported_loss_gpt\n",
    "\n",
    "                    running_count += 1\n",
    "\n",
    "                    print(\"VRAM allocated (MB):\", torch.cuda.memory_allocated()/1e6, \"VRAM reserved (MB):\", torch.cuda.memory_reserved()/1e6)\n",
    "\n",
    "                    break\n",
    "\n",
    "                except RuntimeError as e:\n",
    "                    err = str(e).lower()\n",
    "                    if \"out of memory\" in err or \"cuda out of memory\" in err:\n",
    "                        attempt += 1\n",
    "                        print(f\"[OOM] Epoch {epoch} Step {step_in_epoch} attempt {attempt}/{OOM_RETRY_LIMIT}. Strategy: \", mp_mgr.info())\n",
    "                        cleanup_memory()\n",
    "                        torch.cuda.empty_cache()\n",
    "                        time.sleep(1.0)\n",
    "                        if mp_mgr.use_bf16:\n",
    "                            print(\" -> falling back from bfloat16 to float16 + GradScaler and retrying batch.\")\n",
    "                            mp_mgr.enable_fp16_with_scaler()\n",
    "                            model_kimi.to(device=device, dtype=torch.float16)\n",
    "                            model_gpt.to(device=device, dtype=torch.float16)\n",
    "                            continue\n",
    "                        elif mp_mgr.use_fp16:\n",
    "                            print(\" -> fp16 path OOM; falling back to fp32.\")\n",
    "                            mp_mgr.enable_fp32()\n",
    "                            model_kimi.to(device=device, dtype=torch.float32)\n",
    "                            model_gpt.to(device=device, dtype=torch.float32)\n",
    "                            continue\n",
    "                        elif mp_mgr.use_fp32:\n",
    "                            if attempt >= OOM_RETRY_LIMIT:\n",
    "                                print(\" -> persistent OOM in fp32; re-raising error.\")\n",
    "                                raise\n",
    "                            else:\n",
    "                                print(\" -> retrying after cache clear (fp32).\")\n",
    "                                continue\n",
    "                        else:\n",
    "                            mp_mgr.enable_fp32()\n",
    "                            model_kimi.to(device=device, dtype=torch.float32)\n",
    "                            model_gpt.to(device=device, dtype=torch.float32)\n",
    "                            continue\n",
    "                    else:\n",
    "                        raise\n",
    "\n",
    "            # Logging Training Progress\n",
    "            if global_step % LOG_INTERVAL == 0:\n",
    "                avg_loss_kimi = running_loss_kimi / max(1, running_count)\n",
    "                avg_nll_kimi = running_nll_kimi / max(1, running_count)\n",
    "                avg_aux_kimi = running_aux_kimi / max(1, running_count)\n",
    "                avg_aux_multiplier = running_aux_multiplier / max(1, running_count)\n",
    "                avg_scaled_aux_kimi = running_scaled_aux_kimi / max(1, running_count)\n",
    "                avg_imp_cv_kimi = running_importance_cv_kimi / max(1, running_count)\n",
    "                avg_load_cv_kimi = running_load_cv_kimi / max(1, running_count)\n",
    "                avg_overflow_kimi = running_overflow_kimi / max(1, running_count)\n",
    "                avg_loss_gpt = running_loss_gpt / max(1, running_count)\n",
    "                alloc = torch.cuda.memory_allocated()/1e6 if device.type==\"cuda\" else 0.0\n",
    "                print(f\"Epoch {epoch} Step {step_in_epoch} (Global {global_step}):\")\n",
    "                print(f\"  NanoKimiK2: avg_loss {avg_loss_kimi:.6f} | avg_nll {avg_nll_kimi:.6f} | avg_aux {avg_aux_kimi:.6f} | aux_mult {avg_aux_multiplier:.6f} | scaled_aux {avg_scaled_aux_kimi:.6f} | imp_cv {avg_imp_cv_kimi:.6f} | load_cv {avg_load_cv_kimi:.6f} | overflow% {avg_overflow_kimi:.4f}\")\n",
    "                print(f\"  NanoGPT: avg_loss {avg_loss_gpt:.6f}\")\n",
    "                print(f\"  alloc {alloc:.1f} MB | precision {mp_mgr.autocast_dtype()}\")\n",
    "\n",
    "            # Evaluation\n",
    "            if global_step % EVAL_INTERVAL == 0:\n",
    "                perplexity_kimi, mean_nll_kimi, mean_aux_kimi, mean_bpc_kimi, kimi_stats = evaluate_kimi(model_kimi, val_loader, device, mp_mgr, encoding)\n",
    "                perplexity_gpt, mean_nll_gpt, mean_bpc_gpt, gpt_stats = evaluate_gpt(model_gpt, val_loader, device, mp_mgr, encoding)\n",
    "                val_losses_kimi.append(mean_nll_kimi)\n",
    "                val_losses_gpt.append(mean_nll_gpt)\n",
    "                print(f\"\\nEvaluation at Step {global_step}:\")\n",
    "                print(f\"  NanoKimiK2: Perplexity {perplexity_kimi:.2f}, Mean NLL {mean_nll_kimi:.6f}, Mean Aux {mean_aux_kimi:.6f}, BPC {mean_bpc_kimi:.6f}, Stats {kimi_stats}\")\n",
    "                print(f\"  NanoGPT: Perplexity {perplexity_gpt:.2f}, Mean NLL {mean_nll_gpt:.6f}, BPC {mean_bpc_gpt:.6f}, Eval Loss {gpt_stats.get('mean_nll', float('inf')):.6f}\")\n",
    "\n",
    "            # Story Generation\n",
    "            if global_step % STORY_GEN_INTERVAL == 0:\n",
    "                print(f\"\\nGenerating sample stories at Step {global_step}:\")\n",
    "                for prompt in eval_prompts:\n",
    "                    print(f\"\\nPrompt: {prompt}\")\n",
    "                    completion_kimi = generate_kimi(model_kimi, encoding, prompt, device, mp_mgr, max_tokens=200)\n",
    "                    print(f\"  NanoKimiK2 Completion: {completion_kimi}\")\n",
    "                    completion_gpt = generate_gpt(model_gpt, encoding, prompt, device, mp_mgr, max_tokens=200)\n",
    "                    print(f\"  NanoGPT Completion: {completion_gpt}\")\n",
    "\n",
    "                # Print Training Loss Table\n",
    "                if global_step % 1000 == 0:\n",
    "                    print(\"\\nTraining Loss Table:\")\n",
    "                    print(\"| Segment | NanoKimiK2 Avg Loss | NanoGPT Avg Loss |\")\n",
    "                    print(\"|---------|---------------------|------------------|\")\n",
    "                    for seg in range(1, (global_step // 1000) + 1):\n",
    "                        start = (seg - 1) * 1000\n",
    "                        end = seg * 1000\n",
    "                        avg_kimi = sum(train_losses_kimi[start:end]) / len(train_losses_kimi[start:end]) if len(train_losses_kimi[start:end]) > 0 else 0.0\n",
    "                        avg_gpt = sum(train_losses_gpt[start:end]) / len(train_losses_gpt[start:end]) if len(train_losses_gpt[start:end]) > 0 else 0.0\n",
    "                        print(f\"| {start+50}-{end} | {avg_kimi:.6f} | {avg_gpt:.6f} |\")  # +50 for mid-segment avg\n",
    "\n",
    "                    # Print Validation Loss Table\n",
    "                    print(\"\\nValidation Loss Table:\")\n",
    "                    print(\"| Segment | NanoKimiK2 Avg Val Loss | NanoGPT Avg Val Loss |\")\n",
    "                    print(\"|---------|-------------------------|----------------------|\")\n",
    "                    val_segments_kimi = [val_losses_kimi[i:i+5] for i in range(0, len(val_losses_kimi), 5)]  # 5 evals per 1000 steps (200 interval)\n",
    "                    val_segments_gpt = [val_losses_gpt[i:i+5] for i in range(0, len(val_losses_gpt), 5)]\n",
    "                    for seg, (kimi_seg, gpt_seg) in enumerate(zip(val_segments_kimi, val_segments_gpt), 1):\n",
    "                        avg_kimi_val = sum(kimi_seg) / len(kimi_seg) if len(kimi_seg) > 0 else 0.0\n",
    "                        avg_gpt_val = sum(gpt_seg) / len(gpt_seg) if len(gpt_seg) > 0 else 0.0\n",
    "                        print(f\"| {(seg-1)*1000 + 200}-{seg*1000} | {avg_kimi_val:.6f} | {avg_gpt_val:.6f} |\")\n",
    "\n",
    "# Entry Point\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f3ef7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70d1b2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2e4e12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
